{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import tweepy \n",
    "import json\n",
    "import csv\n",
    "import timeit\n",
    "import time\n",
    "from datetime import datetime\n",
    "from bs4 import BeautifulSoup\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "consumer_key = 'caS0q9PTWurgvlRzlPUbjvKPd'\n",
    "consumer_secret = 'ZOfPz627CepLQAoNcT4Q3YTH6a2bOussTYCA8ti13P5FMKXJPn'\n",
    "access_token = '114510363-3XlfxjFi7cZjVjugcJN6tZt3JLnofP1pFIQd6wmW'\n",
    "access_secret = 'ON79smKdHQkNwxMvDsclgZdHfYX5guJNpqeInw0vosuAQ'\n",
    "\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_secret)\n",
    "\n",
    "api = tweepy.API(auth)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'This is Darla. She commenced a snooze mid meal. 13/10 happens to the best of us https://t.co/tD36da7qLQ'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet = api.get_status(id= 891689557279858688)\n",
    "tweet.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#for status in tweepy.Cursor(api.home_timeline).items():\n",
    "    # Process a single status\n",
    "#    print(status.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prediction_file = \"https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv\"\n",
    "\n",
    "resp = requests.get(prediction_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "decod_file = resp.content.decode('utf-8')\n",
    "file = csv.reader(decod_file.splitlines(), delimiter= '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(\"image-predictions.tsv\", 'wb') as f:\n",
    "    fwriter = csv.writer(f)\n",
    "    for line in file:\n",
    "         fwriter.writerow(line)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prediction = pd.read_csv(\"image-predictions.tsv\")\n",
    "twit_archive = pd.read_csv(\"twitter-archive-enhanced.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calling twitter API for download JSON data for each tweet_id ( API Calling Rate of 800 Calls/15 minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweet_dataList = []\n",
    "def correct_tweetID(data, tweet_dataList):\n",
    "    ctr = 0\n",
    "    ctr2 = 0\n",
    "    start = timeit.default_timer()\n",
    "    print \"Start Time: \", start\n",
    "    for pos,tid in enumerate(data.tweet_id):\n",
    "        try: \n",
    "            tweet_data = api.get_status(id= tid)\n",
    "            tweet_dataList.append(tweet_data._json)\n",
    "            ctr +=1\n",
    "            ctr2 +=1\n",
    "            if ctr == 800:\n",
    "                ctr = 0\n",
    "                print \"Sleeping\"\n",
    "                timeshift = timeit.default_timer() - start\n",
    "                print timeshift\n",
    "                if timeshift < (15*60):\n",
    "                    time.sleep( 15*60  - timeshift + 10)\n",
    "                    print \"AWAKE\"\n",
    "                    start = timeit.default_timer()\n",
    "\n",
    "        ## Id Tweet calls fails, the tweet _id is extracted from twitter text\n",
    "        except Exception as e:\n",
    "\n",
    "            print e\n",
    "            r = re.compile('(\\d{18})')      ## Creating regex for 18-digit Tweet_ID\n",
    "            s = re.search(r, data.loc[pos,'expanded_urls'])     ## Searching and extracting tweet_id from corresponding expanded_url\n",
    "            correct_id = s.group(0)\n",
    "            print \"Old ID: \", tid\n",
    "            data.tweet_id = data.tweet_id.replace(tid, correct_id)          ## Replacing incorrect ID with correct ID\n",
    "            print \"New iD: \", data.loc[pos,'tweet_id']\n",
    "            tweet_data = api.get_status(id= correct_id)          ## API calling with corrected tweet_id to extract JSON data\n",
    "            tweet_dataList.append(tweet_data._json)\n",
    "            print \"ID corrected And Data downloaded\"\n",
    "            print ctr, ctr2,  \" TIME: \", timeit.default_timer()\n",
    "\n",
    "#correct_tweetID(twit_archive)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Saving downloaded JSON data to text file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(\" tweet_json.txt\", 'w') as f:\n",
    "    json.dump(tweet_dataList,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysing twitter_archieve table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "twit_archive.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Inspecting for datatype and null value counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "twit_archive.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be seen that few columns has large amount of NULL values, thesee correspond to retweets and in-reply-to tweets, which are not actual tweet of WeRateDog twitter handle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Inspecting Source values frequency count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "twit_archive.source.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Inspecting rating Numerator and Denominator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "twit_archive.rating_denominator.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "twit_archive.rating_numerator.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It has been found that both numerator and denominator has very wiered values, hence it has to matched with those values in the tweet text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking accuracy of rating_num and rating_denom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "marksstr = re.compile(\"(\\d{1,3}\\.{0,1}\\d{0,4})\\/(\\d{1,3})\") ## Generating Regex for extracting rating numerator score from text\n",
    "for i, text in enumerate(twit_archive.text):\n",
    "    e_num =  re.search(marksstr, text).group(1)\n",
    "    num = twit_archive.loc[i,'rating_numerator']\n",
    "    if int(num) != int(float((e_num))):\n",
    "        print num, e_num, text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Extracting rating denominator score which are incorrect\n",
    "\n",
    "marksstr = re.compile(\"(\\d{1,3}\\.{0,1}\\d{0,4})\\/(\\d{1,3})\")\n",
    "for i, text in enumerate(twit_archive.text):\n",
    "    e_num =  re.search(marksstr, text).group(2)\n",
    "    num = twit_archive.loc[i,'rating_denominator']\n",
    "    if int(num) != int(float((e_num))):\n",
    "        print num, e_num, text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence, it is found that there is some mismatch in the rating-numerator, but not in the rating denominator. Some of values may be looked weired, but that has to be kept as they are extracted from the tweet-text itself. The only examinaotion can be done to re-download the tweet-text dat to check the validity/ accurac of the text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Checking the counts of Dog-Stages i.e. doggo, puppo, pupper and floofer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print twit_archive.doggo.value_counts(), \"\\n\"\n",
    "print twit_archive.puppo.value_counts(), '\\n'\n",
    "print twit_archive.floofer.value_counts(),'\\n'\n",
    "print twit_archive.pupper.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Asserting Dog-stage labels with the corresponding tweet text\n",
    "Now i will programmitically find by reading each tweet to check correct mapping of the dog-stage is done or  not\n",
    "\n",
    "##### Inspecting for 'Doggo'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ctr = 0\n",
    "find_doggo = twit_archive.text.str.find(\"doggo\")\n",
    "print \"index\\tPosition\\t tweet\"\n",
    "for i,pos in enumerate(find_doggo):\n",
    "    if pos != -1:\n",
    "        print i,\"\\t\", twit_archive.doggo[i],\"\\t\", twit_archive.text[i]\n",
    "        ctr +=1\n",
    "print \"Total counts: \",ctr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Inspecting for 'floofer'\n",
    "By visual inspection it has been found, floofer dog, is many times refferred as a 'floof' in the tweets, hence will search using 'floof' keyword rather than 'floofer'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ctr = 0\n",
    "find_floof = twit_archive.text.str.find(\"floof\")\n",
    "print \"index\\tPosition \\t tweet\"\n",
    "for i,pos in enumerate(find_floof):\n",
    "    if pos != -1:\n",
    "        print i,\"\\t\", twit_archive.floofer[i],\"\\t\", twit_archive.text[i]\n",
    "        ctr +=1\n",
    "print \"Total counts: \",ctr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Inspecting for 'pupper'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ctr = 0\n",
    "find_pupper = twit_archive.text.str.find(\"pupper\")\n",
    "print \"index\\tPosition \\t tweet\"\n",
    "for i,pos in enumerate(find_pupper):\n",
    "    if pos != -1:\n",
    "        print i,\"\\t\", twit_archive.pupper[i],\"\\t\", twit_archive.text[i]\n",
    "        ctr +=1\n",
    "print \"Total counts: \",ctr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Inspecting for 'puppo'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ctr = 0\n",
    "find_puppo = twit_archive.text.str.find(\"puppo\")\n",
    "print \"index\\tDogStage\\t tweet\"\n",
    "for i,pos in enumerate(find_puppo):\n",
    "    if pos != -1:\n",
    "        print i,\"\\t\", twit_archive.puppo[i],\"\\t\", twit_archive.text[i]\n",
    "        ctr +=1\n",
    "print \"Total counts: \",ctr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Conclusion\n",
    "Hence it can be concluded, that there many cases for each dog stage which are not correctly mapped and hence is needed to be corrected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Checking Expanded urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "twit_archive.expanded_urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Tweets which are not from WeRateDogs twitter handle\n",
    "By visual inspection, it is found that there re many tweets which are from other sources but not WeRateDog twitter handle, hence it is required to programatically access number of such cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "twit_archive[twit_archive.expanded_urls.str.find('dog_rates') == -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "twit_archive[twit_archive.expanded_urls.isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some non-twitter links and lot of NaNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. Assessing tweet JSON data for a sample iD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweet = api.get_status(id= 706165920809492480)\n",
    "tweet._json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweet._json.keys()  ### Checking for main keys of the JSON data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9. Checking for accuracy of dog names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "twit_archive[twit_archive.name.str.len() <=2].name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence it can be concluded it looks many dogs names are wrongly extracted from the tweet. Especially names like 'a', 'an', 'my' are wrongly chosen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10. Assessing prediction table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prediction.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "prediction.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prediction.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prediction[prediction.p1== 'starfish']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 1. Quality\n",
    "#### 1. twitter-archive\n",
    "\n",
    "  1. The Retweets all correspond to the duplicity of original tweets of WeRateDogs. Hence must be removed.\n",
    "  2. Tweets corresponding to valid 'in_reply_to_status_id' data are not original review of Dog by WeRateDog, nor it has link for it.\n",
    "  3. Erroneous data types - Datetime, Dog catagories - Doggo, puppo etc are string object.\n",
    "  4. Wrongs IDs - Some of the Tweet IDs are wrongly mentioned.\n",
    "  5. The source column, has a html element of HTTP link. It must have only the text value of html element.\n",
    "  6. The source column must be of catagory data type\n",
    "  7. Some of the values of the 'Rating numerator' and 'Rating denominator' has junk values\n",
    "  8. The dog stage - doggo, floofer, puppo, pupper is also not correctly mapped.\n",
    "  9. The dog name has sometimes invalid values.\n",
    "  10. There were lots of NaN value in expanded_url columns.\n",
    "  11. There is lot of data that is tweets not fron the twitter handle of 'WeRateDogs' but others or from WeRateDog VINE.CO website, which is not a tweet. They all has to be removed as it is required to Wrangle only We Rate Dogs original twitter tweets\n",
    "  12. tweet_id must be sorted in one direction.\n",
    "\n",
    "#### 2. prediction table\n",
    "  1. The tweet_ids must be sorted in one direction : ascending or descending\n",
    " \n",
    "  3. The prediction table must consist of only tweet IDs corresponding to those in twitter_archive_master table.\n",
    "\n",
    "\n",
    "\n",
    "### 2. Tidyness\n",
    "\n",
    "  1. The column for retweet IDs and in-reply-to-IDs is not desired and required, as it creates duplicit, and hence be removed\n",
    "  2. The twitter data for 'retweets_count' and 'likes_count' has to merged with the table.\n",
    "  3. The dog type columns i.e doggo, floofer, puppo, pupper must be in single column named as Dog type\n",
    "  4. The prediction column must be part of twitter_archive master table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Creating copy\n",
    "bc = twit_archive.copy()\n",
    "twit_archive.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning\n",
    "\n",
    "### 1.1.1(Quality) Removing Retweets \n",
    "#### Define\n",
    "Retweets actually creating duplicity in the dataset, as they are retweeting their own tweets. Sometimes they are just retweeting someone else tweets, which is actually not a Dog's Rating. Hence we simply delete all the rows which are the retweets by WeRateDog twitter handle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Deleting all the rows which has a valid retweet ID.\n",
    "bc1 = bc[~bc.retweeted_status_id.notnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bc1.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.2 (Quality) Removing in_reply_to tweets\n",
    "#### Define\n",
    " Reply tweets also do not are the actual dog ratings tweets, hence they can be removed from the table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Deleting rows with valid 'in_reply_to_status_id' data.\n",
    "bc2 = bc1[~bc1.in_reply_to_status_id.notnull()]\n",
    "\n",
    "bc2 = bc2.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bc2.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 (Tidy) Remove columns correponding to reply_to and re_tweets IDs and status_ids\n",
    "### 1.1.10 (Quality) Lot of NaN in 'expanded_urls' column in twitter_archive table\n",
    "#### Define\n",
    "Since the retweet data and in-reply-to data has been removed, the need for the corresponding column is not required. Hence these column can be easily droped. Such tweets data has all 'NaN' in the expanded_url section. It is assumed that every tweet data must has its url link for reference. Hence the tweets IDs having 'NaN' in expanded must be removed. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bc2 = bc2.drop(['in_reply_to_status_id', 'in_reply_to_user_id', 'retweeted_status_id',\n",
    "          'retweeted_status_user_id','retweeted_status_timestamp'], axis = 1)\n",
    "\n",
    "# Looking for number of tweet IDs with NaN in expanded_url column. \n",
    "bc2[bc2.expanded_urls.isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bc3 = bc2[~bc2.expanded_urls.isnull()]  #Removing above tweets data from the table\n",
    "bc3 = bc3.reset_index(drop = True)      #Reseting the index of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bc2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bc3.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bc3.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.3(Quality) Changing timestamp datatype to Datetime\n",
    "#### Define\n",
    "The datetime data column is string-type. But for analysis in later stage programtically, datatime data must be of type 'Dateime' category."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    bc3.timestamp = bc3.timestamp.apply(lambda x: datetime.strptime(x[:-6],'%Y-%m-%d %H:%M:%S'))\n",
    "except Exception as e:\n",
    "     print e\n",
    "             "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bc3.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.4(Quality) Correcting the incorrect tweet IDs\n",
    "### 2.2(Tidyness)  Extracting 'favorite_count' and 'retweet_count' from the downloaded json data and adding them as column to twitter_archive table.\n",
    "\n",
    "#### Define\n",
    "It has been found that some of the tweet IDs are incorrect or are not callable using twitter API. So i will start downloading the json data by calling twitter API one by one. If any failure occur, i will extract the tweet ID from the 'expanded_url' and again make a twitter API call.\n",
    "The downloaded JSON data will stored in the form of list and is then saved to a text file - 'tweet_json.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(\"tweet_json.txt\", 'r') as f:\n",
    "    json_tweetDataList = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "retweet_count_list = []\n",
    "favorite_count_list = []\n",
    "i=0\n",
    "for jsonData in json_tweetDataList:\n",
    "    if str(jsonData['id']) == str(bc3.tweet_id[i]) :\n",
    "        retweet_count_list.append(jsonData[\"retweet_count\"])\n",
    "        favorite_count_list.append(jsonData[\"favorite_count\"])\n",
    "        i+=1\n",
    "       \n",
    "len(retweet_count_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating column for 'retweet_count' and 'favorite_count'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bc3[\"retweet_count\"] = pd.Series(retweet_count_list)\n",
    "bc3[\"favorite_count\"] = pd.Series(favorite_count_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#### Temporarily saving the data to a csv-file\n",
    "bc3.to_csv(\"tweet123.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bc3.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bc3.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.5(Quality) The source column, has a html element of HTTP link. It must have only the text value of html element.\n",
    "### 1.1.6(Quality) The source column must be of type catagory\n",
    "\n",
    "#### Define\n",
    "The source column has value represented in XML element. The only important content is the 'text' value of that element. Thid can be done by first converting the string data into html -datatype. And subsequently extracting text value of that element.\n",
    "Then coverting the datatype of column to 'category' type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##Editing Source\n",
    "\n",
    "bc3.source = bc3.source.apply(lambda x: BeautifulSoup(x, 'lxml').text)\n",
    "bc3.source = bc3.source.astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bc3.source.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bc3.sample(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.7 (Quality) Correcting Rating: Numerator and Denominator\n",
    "\n",
    "#### Define\n",
    "We can extract the correct marks from the tweet status itself and assigned to numerator and denominator column. It has been found that the numerator can be floating point number also. Hence we can create 'Regex' value for different range of values for the numerator and subsequently use it to extract the score from the numerator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bc3['rating_numerator'] = bc3['rating_numerator'].astype(float)\n",
    "marksstr = re.compile(\"(\\d{1,3}\\.{0,1}\\d{0,4})\\/(\\d{1,3})\")\n",
    "for i, text in enumerate(bc3.text):\n",
    "    e_num =  re.search(marksstr, text).group(1)\n",
    "    num = bc3.loc[i,'rating_numerator']\n",
    "    if int(num) != int(float((e_num))):\n",
    "        bc3.loc[i, 'rating_numerator'] = e_num\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bc3.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.8 (Quality)Error in correctly mapping of dog catagory 'Floofer' or 'floof', 'doggo' etc in some IDs\n",
    "It has been found that there are multiple instances, that a dog-stage catagory is present in the tweet-text, slightly in different form, but the corresponding marked as none. This can be done by reading each tweet's ID text, and searching for the desired catagory name in most general form."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "for i,txt in  enumerate(bc3.text.str.lower()): ##\n",
    "    if txt.find(\"floof\") != -1 :\n",
    "        #print bc3.loc[i, 'floofer'], txt\n",
    "        bc3.loc[i,'floofer'] = 'floofer'\n",
    "        bc3.loc[i,'doggo'] = 'None'\n",
    "        bc3.loc[i,'puppo'] = 'None'\n",
    "        bc3.loc[i,'pupper'] = 'None'\n",
    "        \n",
    "    elif txt.find(\"doggo\") != -1 :\n",
    "        bc3.loc[i,'doggo'] = 'doggo'\n",
    "        bc3.loc[i,'puppo'] = 'None'\n",
    "        bc3.loc[i,'pupper'] = 'None'\n",
    "        bc3.loc[i,'floofer'] = 'None'\n",
    "        \n",
    "    elif txt.find(\"puppo\") != -1 :\n",
    "        bc3.loc[i,'puppo'] = 'puppo'\n",
    "        bc3.loc[i,'floofer'] = 'None'\n",
    "        bc3.loc[i,'doggo'] = 'None'\n",
    "        bc3.loc[i,'pupper'] = 'None'\n",
    "        \n",
    "    elif txt.find(\"pupper\") != -1 :\n",
    "        bc3.loc[i,'pupper'] = 'pupper'\n",
    "        bc3.loc[i,'floofer'] = 'None'\n",
    "        bc3.loc[i,'doggo'] = 'None'\n",
    "        bc3.loc[i,'puppo'] = 'None'\n",
    "    \n",
    "    else:\n",
    "        bc3.loc[i,'doggo'] = '-'\n",
    "        bc3.loc[i,'pupper'] = '-'\n",
    "        bc3.loc[i,'puppo'] = '-'\n",
    "        bc3.loc[i,'floofer'] = '-'\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test \n",
    "It can be seen from below output, the no. of IDs for which no catagory information be extracted is labelled as '-', and is equal to 1694 for all the cases. The counts of individual dog-stage is also increased from previous one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print '\\n-- Catagory Doggo -- \\n', bc3.doggo.value_counts() \n",
    "print '\\n-- Catagory puppo -- \\n', bc3.puppo.value_counts()\n",
    "print '\\n-- Catagory pupper -- \\n', bc3.pupper.value_counts()\n",
    "print '\\n-- Catagory floofer -- \\n', bc3.floofer.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bc3.floofer.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.3(Quality) Dog catagories - Doggo, puppo etc are string object, must be of type catagory\n",
    "\n",
    "#### Define\n",
    "Dog-stage type in the table are set as string-type. But for furthur analysis and data exploration, it would be better if it is set as category-type. It can easily done by using 'astype' function in pandas dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bc3.doggo = bc3.doggo.astype('category')\n",
    "bc3.pupper = bc3.pupper.astype('category')\n",
    "bc3.puppo = bc3.puppo.astype('category')\n",
    "bc3.floofer = bc3.floofer.astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bc3.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Saving data in temperory csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bc3.to_csv('tweetsfilter2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.9 (Quality) Many dogs names are not mapped accurately. Needed to be correct. \n",
    "\n",
    "#### Define\n",
    "For this we have to go through the tweet text data througout the table, and needed to evaluate if there is any consistency in part of text that has dog name present in there. Since there are so many incorrect names extracted, it is required to be done programetically. I am doing this, by re-extracting dog names for all the tweet IDs, whereever possible, though there still must be some errors. \n",
    "Based on my analysis of tweet text, i have shortlisted 4 types of string-text, that has name in it.\n",
    "Such string start as :\n",
    "1. \"named ....\"\n",
    "2. \"name is ...\"\n",
    "3. \"This is ...\" (Excluding \"This is a ...\")\n",
    "4. \"say hello to ...\"\n",
    "\n",
    "I will look for above strings case by case in each text, and extract the dog name from it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bc4 = bc3.copy()\n",
    "nameList = []\n",
    "for i,text in enumerate(bc4.text):\n",
    "    #Case 1: Eg - This is a Lofted Aphrodisiac Terrier named Kip. Big fan of bed n breakfasts. Fits perfectly.\n",
    "    if (text.find('named')) != -1 :\n",
    "        correct_name = text.split('named ')[1].split('.')[0].split(' ')[0]\n",
    "    \n",
    "    #Case 2: Eg- This is my dog. Her name is Zoey. She knows I've been rating other dogs.\n",
    "    elif (text.find('name is ')) != -1 :\n",
    "        correct_name = text.split('name is ')[1].split('.')[0].split(' ')[0] \n",
    "        #print text\n",
    "        \n",
    "    #Case 3: Eg- This is Bruno. He is a service shark. Only gets out of the water to assist you.\n",
    "    elif (text.split('.')[0].find('This is ')) != -1 :\n",
    "        #print text\n",
    "        if (text.find('This is a ')) == -1 :\n",
    "            correct_name = text.split('This is ')[1].split('.')[0].split(' ')[0]\n",
    "        else: \n",
    "            correct_name = 'None'\n",
    "    \n",
    "    #Case 4: Eg- Meet Elliot. He's a Canadian Forrest Pup. Unusual number of antlers for a dog. \n",
    "    elif (text.split('.')[0].find('Meet ')) != -1 :\n",
    "        correct_name = text.split('Meet ')[1].split('.')[0].split(' ')[0]\n",
    "        #print text\n",
    "    \n",
    "    #Case 5: Eg- Say hello to Quinn. She's quite the goofball. Not even a year old.\n",
    "    elif (text.split('.')[0].lower().find('say hello to ')) != -1 :\n",
    "        #print text\n",
    "        correct_name = text.lower().split('say hello to ')[1].split('.')[0].split(' ')[0].title()\n",
    "        #print correct_name, text\n",
    "       \n",
    "    #Case 6: Eg- Here is George. George took a selfie of his new man bun and that is downright epic\n",
    "    elif (text.split('.')[0].lower().find('here is ')) != -1 :\n",
    "        #print text\n",
    "        if len(text.split('.')[0].split(' '))== 3:\n",
    "            correct_name = text.lower().split('here is ')[1].split('.')[0].split(' ')[0].title()\n",
    "        else : \n",
    "            correct_name = 'None'\n",
    "            \n",
    "    \n",
    "    else: # No name identified\n",
    "        correct_name = 'None'\n",
    "\n",
    "    \n",
    "    nameList.append(re.split('[,.]',correct_name)[0])\n",
    "\n",
    "bc4['name'] = pd.Series(nameList)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ctr = 0\n",
    "print \"Old Name\\tCorrected Name\"\n",
    "for i, name in enumerate(bc4.name):\n",
    "    if bc4.name[i] != bc3.name[i] :\n",
    "        print bc3.name[i],'\\t       ', bc4.name[i], '\\t',' ',bc4.text[i]\n",
    "        ctr +=1\n",
    "print \"Total Correction :\", ctr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "It can be seen fro mthe output above, most of the wrongly extracted names are corrected with correct name if available or changed to 'None', if no name is available with the total number of correction = 94, which is significant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 (Tidyness) The dog type columns i.e doggo, floofer, puppo, pupper must be merged into single column named as Dog Stage\n",
    "#### Define\n",
    "All of the labels of Dog Stage i.e. doggo, floofer, puppo, pupper or None are of same catagory type. Hence thwy must be merged into dsinglw column named 'DogStage' which shows these stages for different IDs. This can be done using pandas 'melt' function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bc5 = pd.melt(bc4, id_vars=['tweet_id','timestamp','source','text','expanded_urls','rating_numerator','rating_denominator','name','retweet_count','favorite_count'], value_vars= ['doggo','floofer','pupper','puppo'], var_name='Dog Stage')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bc5.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Since '-' is a case when there is no dog stage available, hence there we multiple (4 times) overlapping of this data. It has to kept only once, and rest all removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bc5.drop(bc5[2094:].index[bc5[2094:].value == '-'], inplace= True)  ## Removing extra cases of '-' catagory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bc5.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Removing cases, 3 out 4 catagory is 'Null', as it brings huge overlapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bc5 = bc5.drop(bc5.index[bc5.value == 'None'])\n",
    "bc5 = bc5.drop(['Dog Stage'], axis = 1)\n",
    "bc5 = bc5.rename(columns = {'value':'Dog_Stage'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bc5.Dog_Stage.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "bc5.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.11 Eliminating all tweets data other than from WeRateDogs\n",
    "#### Define\n",
    "Since we have to analyse only tweets (No retweet) of WeRateDog twitter handle, all other tweets must be removed from the table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "bc5[bc5.expanded_urls.str.find('dog_rates') == -1]    \n",
    "## Extracting tweets which donot have keyword 'dog_rates' in expanded_urls column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The above are all those tweets that is not from 'WeRateDog' twitter handle but others.\n",
    "bc6 = bc5[bc5.expanded_urls.str.find('dog_rates') != -1]\n",
    "bc6 = bc6.reset_index(drop= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bc6.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bc6.to_csv(\"tweetfilter3.csv\")\n",
    "bc6.tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.12 Sorting twitter_archive table by tweet_id\n",
    "#### Define\n",
    "Since all the tweets are jumbled up, they must be sorted in one direction, which help in easy visual/programatic tracing of an ID. So sorting in ascending order."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### 1.2.1 Sorting twitter \n",
    "bc6 = bc6.sort_values('tweet_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bc6.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bc6.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred = pd.read_csv(\"image-predictions.tsv\")\n",
    "pred.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1 The prediction table must be sorted w.r.t tweet_id\n",
    "### 1.2.2 The prediction table must have only tweet IDs corresponding to those in twitter_archive table.\n",
    "#### Define\n",
    "The prediction table has 2074 observations, but 'twitter_archive_master.csv' has 1971 observation. Hence the prediction table must have only those IDs which are there in twitter_archive_master table. \n",
    "This can be done first extracting those IDs which are their in the master table, and then finally eliminating all other remaining IDs. The final filtered table must be sorted for easy tracebility of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Analysing matching tweet_ids\n",
    "ctr = 0\n",
    "id_list = []\n",
    "for index, tid in enumerate(pred.tweet_id):\n",
    "    \n",
    "    if tid in list(bc6.tweet_id):\n",
    "        id_list.append(index)\n",
    "        ctr +=1\n",
    "ctr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred2 = pred.iloc[id_list]\n",
    "pred2 = pred2.reset_index(drop = True)\n",
    "pred2 = pred2.sort_values('tweet_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print pred2.info()\n",
    "print bc6.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pred2.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 One of prediction test must be part of twitter_archive master table\n",
    "#### Define\n",
    "The test results of prediction algoritham must be part of twitter_archive_master table for a complete analysis and making data more tidy. Hence i will merged the data of first prediction test i.e. 'p1', 'p1_conf' and 'p1_dog' with the main table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Extracting the specific columns for first prediction test\n",
    "pred2 = pred2.loc[:,['tweet_id', 'p1','p1_conf', 'p1_dog']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Merging the table over the column 'tweet_id'\n",
    "\n",
    "bc6['tweet_id'] = bc6.tweet_id.astype('int64')  # Ensuring both column have same datatype\n",
    "\n",
    "twitter_archive_final = pd.merge(bc6, pred2, on= 'tweet_id')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "twitter_archive_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "twitter_archive_final.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Saving the final master data to csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "twitter_archive_final.to_csv('twitter_archive_master.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Insight & Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweet_data = pd.read_csv('twitter_archive_master.csv')\n",
    "pd.options.display.max_colwidth = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweet_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweet_data.Dog_Stage = tweet_data.Dog_Stage.apply(lambda x: 'ND' if x == '-' else x)\n",
    "tweet_data.Dog_Stage.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Tweet with maximum re-tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "max_pos = tweet_data.retweet_count.argmax()\n",
    " \n",
    "tweet_data.iloc[max_pos]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Tweet with maximum like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "max_pos = tweet_data.favorite_count.argmax()\n",
    " \n",
    "tweet_data.iloc[max_pos]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Number of tweets with incorrect Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweet_data.p1_dog.value_counts()[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence no. incorrect predictions = 504"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Tweet with highest rating and least rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rating = tweet_data.rating_numerator/ tweet_data.rating_denominator\n",
    "print \"************************HIGHEST RATING***********************\"\n",
    "tweet_data.iloc[rating.argmax()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rating = tweet_data.rating_numerator/ tweet_data.rating_denominator\n",
    "print \"************************LEAST RATING***********************\"\n",
    "tweet_data.iloc[rating.argmin()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Prediction Accuracy of Neural algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Predicition accuracy = Total True cases / Total cases\n",
    "true_cases = tweet_data.p1_dog.value_counts()[1]\n",
    "total = len(tweet_data)\n",
    "accuracy = 100.0 *true_cases/total\n",
    "\n",
    "print \"Accuracy = \", accuracy"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from ggplot import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. Bar PLot for Dog Stage Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data2 = tweet_data[tweet_data.Dog_Stage != 'ND']\n",
    "ggplot(aes(x ='tweet_data.Dog_Stage'), data = data2) + geom_bar() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. Trends for no. of likes vs Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#tweet_data\n",
    "#tweet_data.timestamp = tweet_data.timestamp.apply(lambda x: datetime.strptime(x,'%Y-%m-%d %H:%M:%S'))\n",
    "\n",
    "ggplot(aes(x ='tweet_data.timestamp', y = 'tweet_data.favorite_count'), data = tweet_data) + \\\n",
    "geom_point(alpha =1) + stat_smooth(color = 'blue') +\\\n",
    "scale_x_date(labels='%m-%y')+ \\\n",
    "ylim(0,50000) +\\\n",
    "xlab('Time') + ylab('No. of Likes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. Trends for no. of retweets vs Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ggplot(aes(x ='tweet_data.timestamp', y = 'tweet_data.retweet_count'), data = tweet_data) +\\\n",
    "geom_point() + stat_smooth(color = 'red') +\\\n",
    "ylim(0,12000) + scale_x_date(labels='%m-%y') +\\\n",
    "xlab('Time') + ylab('No. of Retweets')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be concluded from 2,3 and 4 that overall popularity of WeRateDogs twitter page, has increased significantly over time which is indicated by the increase in the amount of average no. of retweets and average no. of like to a given post/tweet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4. No. of retweets over time for different dog stages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ggplot(aes(x ='tweet_data.timestamp', y = 'tweet_data.retweet_count', color = 'Dog_Stage'), data = data2) +\\\n",
    "geom_point() + stat_smooth(se = 'False') +\\\n",
    "scale_color_brewer(type = 'qual', palette = 2) +\\\n",
    "ylim(0,12000) + scale_x_date(labels='%m-%y') +\\\n",
    "xlab('Time') + ylab('No. of Retweets')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5. Rating histogram for different Dog-stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "ggplot(aes(x ='tweet_data.rating_numerator/ tweet_data.rating_denominator', fill = 'Dog_Stage' ),\\\n",
    "       data = data2) +\\\n",
    "geom_histogram(binwidth = 0.03)  + xlim(0,1.5)+\\\n",
    "xlab('Equivalent Rating') + ylab('Count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be seen that the 'doggo' category has recieved more of the high rating value compared to others. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
